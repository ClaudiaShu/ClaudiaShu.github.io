---
layout: page
title: "Unsupervised hard Negative Augmentation for contrastive learning"
date: 2024-01-22
permalink: /publications/2024-arXiv-UNA/
venue: "preprint"
year: 2024
paper_url: "https://arxiv.org/pdf/2401.02594"
code_url: "https://github.com/ClaudiaShu/UNA"
abbrev: "UNA"
---

<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3c.org/TR/1999/REC-html401-19991224/loose.dtd">
<html xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" lang="en">

<head>


<title>Shu: UNA</title>

<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>

</head>

<body>

<div id="primarycontent">
<center><h3><a class=link href="https://claudiashu.github.io/">Yuxuan Shu</a>&nbsp;&nbsp;&nbsp;<a class=link href="https://lampos.net/home">Vasileios Lampos</a></h3></center>

<center><h4><strong>{{ page.venue }} {{ page.year }}</strong></h4></center>
	
<center><h4><strong><a href="{{ page.code_url }}"> Code </a>&nbsp;&nbsp;|&nbsp;&nbsp;<a href="{{ page.paper_url }}"> Paper </a></strong></h4></center> 


<!--Abstract-->
<div class="container" style="padding-top: 20px;">
<h2><strong>Abstract</strong></h2>
<p style='text-align: justify; '>
  We present Unsupervised hard Negative Augmentation (UNA), a method that generates synthetic negative instances based on the term frequency-inverse document frequency (TF-IDF) retrieval model. UNA uses TF-IDF scores to ascertain the perceived importance of terms in a sentence and then produces negative samples by replacing terms with respect to that. Our experiments demonstrate that models trained with UNA improve the overall performance in semantic textual similarity tasks. Additional performance gains are obtained when combining UNA with the paraphrasing augmentation. Further results show that our method is compatible with different backbone models. Ablation studies also support the choice of having a TF-IDF-driven control on negative augmentation. 
</div>
	

<!--Highlights-->
<div class="container" style="padding-top: 20px;">
<h2><strong>Highlights</strong></h2><hr>
<p style='text-align: justify; '>
  We therefore propose Unsupervised hard Negative Augmentation (UNA), an augmentation strategy for generating negative samples in Self-supervised contrastive learning.
  The proposed method is driven by TF-IDF to generate hard negative pairs. Words with more substance have a greater probability of being swapped, and more common words do not.
	
	
<!--Datasets-->
<div class="container" style="padding-top: 20px;">
<h2><strong>Datasets</strong></h2>
	
	We pretrained on an English Wikipedia corpus (containing 1 million sentences) proposed by <a href="https://github.com/princeton-nlp/SimCSE">SimCSE</a>
  and evaluated on seven Semantic Textual Similarity tasks, which are STS 2012-2016, STS Benchmark, and SICK Relatedness. All data sets can be found in the official code base of SimCSE.
  
</div>

<!-- citation -->
<div class="container" style="padding-top: 20px;">
  <h2 id="citation" style="padding-top: 80px; margin-top: -80px;"><strong>Citation</strong></h2><hr>
  <pre style="padding: 1.0rem; margin-right: 0; margin-left: 0; font-size:12px">
@article{shu2024unsupervised,
  title={Unsupervised hard Negative Augmentation for contrastive learning},
  author={Shu, Yuxuan and Lampos, Vasileios},
  journal={arXiv preprint arXiv:2401.02594},
  year={2024}
}</pre>

</div>
	

<!-- Footer -->
<div class="container" style="padding-top: 50px;">
    <center>
      <footer>
        <p>Â© Yuxuan Shu {{ page.year }}</p>
      </footer>
    </center>
</div>


</body></html>